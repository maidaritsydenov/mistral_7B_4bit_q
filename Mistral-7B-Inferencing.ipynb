{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Inferencing on Mistral 7B LLM with 4-bit quantization ðŸš€\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anaconda3\\envs\\myenv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:15<00:00,  1.44s/it]\n",
      "Downloading (â€¦)okenizer_config.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 963/963 [00:00<00:00, 481kB/s]\n",
      "Downloading tokenizer.model: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 493k/493k [00:00<00:00, 2.73MB/s]\n",
      "Downloading (â€¦)/main/tokenizer.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.80M/1.80M [00:00<00:00, 2.62MB/s]\n",
      "Downloading (â€¦)cial_tokens_map.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 72.0/72.0 [00:00<00:00, 36.0kB/s]\n"
     ]
    }
   ],
   "source": [
    "def load_quantized_model(model_name: str):\n",
    "    \"\"\"\n",
    "    :param model_name: ÐÐ°Ð·Ð²Ð°Ð½Ð¸Ðµ Ð¸Ð»Ð¸ Ð¿ÑƒÑ‚ÑŒ Ðº Ð·Ð°Ð³Ñ€ÑƒÐ¶Ð°ÐµÐ¼Ð¾Ð¹ Ð¼Ð¾Ð´ÐµÐ»Ð¸.\n",
    "    :return: Ð—Ð°Ð³Ñ€ÑƒÐ¶ÐµÐ½Ð½Ð°Ñ ÐºÐ²Ð°Ð½Ñ‚Ð¾Ð²Ð°Ð½Ð½Ð°Ñ Ð¼Ð¾Ð´ÐµÐ»ÑŒ.\n",
    "    \"\"\"\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16\n",
    "    )\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        load_in_4bit=True,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        quantization_config=bnb_config\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "def initialize_tokenizer(model_name: str):\n",
    "    \"\"\"\n",
    "    Ð˜Ð½Ð¸Ñ†Ð¸Ð°Ð»Ð¸Ð·Ð°Ñ†Ð¸Ñ Ñ‚Ð¾ÐºÐµÐ½Ð¸Ð·Ð°Ñ‚Ð¾Ñ€Ð° Ñ ÑƒÐºÐ°Ð·Ð°Ð½Ð½Ñ‹Ð¼ Ð¸Ð¼ÐµÐ½ÐµÐ¼ Ð¼Ð¾Ð´ÐµÐ»Ð¸.\n",
    "\n",
    "    :param model_name: ÐÐ°Ð·Ð²Ð°Ð½Ð¸Ðµ Ð¸Ð»Ð¸ Ð¿ÑƒÑ‚ÑŒ Ðº Ð¼Ð¾Ð´ÐµÐ»Ð¸ Ð´Ð»Ñ Ð¸Ð½Ð¸Ñ†Ð¸Ð°Ð»Ð¸Ð·Ð°Ñ†Ð¸Ð¸ Ñ‚Ð¾ÐºÐµÐ½Ð¸Ð·Ð°Ñ‚Ð¾Ñ€Ð°.\n",
    "    :return: Ð˜Ð½Ð¸Ñ†Ð¸Ð°Ð»Ð¸Ð·Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð½Ñ‹Ð¹ Ñ‚Ð¾ÐºÐµÐ½Ð¸Ð·Ð°Ñ‚Ð¾Ñ€.\n",
    "    \"\"\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    tokenizer.bos_token_id = 1  # Ð£ÑÑ‚Ð°Ð½Ð°Ð²Ð»Ð¸Ð²Ð°ÐµÐ¼ Ð¸Ð´ÐµÐ½Ñ‚Ð¸Ñ„Ð¸ÐºÐ°Ñ‚Ð¾Ñ€ Ð½Ð°Ñ‡Ð°Ð»Ð° Ð¿Ñ€ÐµÐ´Ð»Ð¾Ð¶ÐµÐ½Ð¸Ñ\n",
    "    return tokenizer\n",
    "\n",
    "\n",
    "\n",
    "stop_token_ids = [0] # ÐžÐ¿Ñ€ÐµÐ´ÐµÐ»ÑÐµÐ¼ Ð¸Ð´ÐµÐ½Ñ‚Ð¸Ñ„Ð¸ÐºÐ°Ñ‚Ð¾Ñ€Ñ‹ Ð¾ÑÑ‚Ð°Ð½Ð¾Ð²ÐºÐ¸\n",
    "model_name = 'bn22/Mistral-7B-Instruct-v0.1-sharded'\n",
    "model = load_quantized_model(model_name)\n",
    "tokenizer = initialize_tokenizer(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer(text, max_new_tokens=200):\n",
    "    text = f\"[INST] {text} [/INST]\"\n",
    "    encoded = tokenizer(text, return_tensors=\"pt\", add_special_tokens=False)\n",
    "    model_input = encoded\n",
    "    generated_ids = model.generate(**model_input, max_new_tokens=max_new_tokens, do_sample=True)\n",
    "    decoded = tokenizer.batch_decode(generated_ids)\n",
    "    answer = decoded[0]\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "d:\\Anaconda3\\envs\\myenv\\lib\\site-packages\\transformers\\generation\\utils.py:1591: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cpu, whereas the model is on cuda. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cuda') before running `.generate()`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------\n",
      "ANSWER: [INST] \n",
      "You are genius Data Scientist and Python Developer.\n",
      "Write a code that predicts the user's future 10 purchases (mcc-codes) based on his purchase history consisting of mcc codes in chronological order.\n",
      " [/INST] Certainly! To predict a user's future 10 purchases, we can use a machine learning approach called \"sequence classification\". Here's a high-level overview of the code I would write to accomplish this task:\n",
      "\n",
      "1. Pre-process the purchase history dataset to normalize or standardize the data, and convert the mcc codes into numerical form.\n",
      "2. Define the input sequence length. In this case, we'll assume the user's purchase history is sequential, with each purchase being recorded in chronological order. So we could set the input sequence length to be the maximum number of purchases in the user's history, plus the number of additional time steps we want to predict. For example, if the user has made 10 purchases, we could predict the next 10 purchases (10 total).\n",
      "3. Define a neural network model for sequence classification. In this case, we could use a recurrent neural network (RNN) such as a Long Short-Term Memory (LSTM) model. LSTMs are well-suited for sequence modeling tasks because they can learn to maintain an internal state over time and incorporate new information as it becomes available.\n",
      "4. Train the model on the pre-processed dataset. We will split the dataset into training and validation sets, and use the training set to train the model. We will use the validation set to tune hyperparameters such as learning rate, batch size, and number of epochs.\n",
      "5. Evaluate the model on a test set. Once the model is trained and tuned, we will use it to make predictions on a separate test set, which will allow us to evaluate the model's performance on unseen data.\n",
      "6. Use the model to predict the user's future purchases. On the test set, we will first feed the user's past purchases into the model, up to the maximum input sequence length we set. Then we can use the RNN model to predict the next purchase in the sequence. Finally, we can repeat this process 10 times, predicting the next 10 purchases in sequence, to get the user's next 10 purchases.\n",
      "\n",
      "Here's some sample code in Python using the Keras deep learning library and TensorFlow backend:\n",
      "```\n",
      "import numpy as np\n",
      "from keras.models import Sequential\n",
      "from keras.layers import Embedding, LSTM, Dense\n",
      "from keras.utils import np_utils, to_categorical\n",
      "\n",
      "# Define input sequence length\n",
      "MAX_SEQUENCE_LENGTH = 10\n",
      "\n",
      "# Define input sequence mask matrix\n",
      "SEQUENCE_MASK = np.zeros((MAX_SEQUENCE_LENGTH, 1))\n",
      "for i in range(1, MAX_SEQUENCE_LENGTH + 1):\n",
      "  SEQUENCE_MASK[i-1] = 1\n",
      "\n",
      "# Define input embedding matrix\n",
      "EMBEDDING_DIM = ... # TODO: specify embedding dimension\n",
      "X = np.random.randn(MAX_SEQUENCE_LENGTH, EMBEDDING_DIM)\n",
      "\n",
      "# Define neural network model\n",
      "model = Sequential()\n",
      "model.add(Embedding(MAX_SEQUENCE_LENGTH, EMBEDDING_DIM, input_length=X.shape[1]))\n",
      "model.add(LSTM(200))\n",
      "model.add(Dense(1, activation='sigmoid'))\n",
      "\n",
      "# Define optimizer and loss function\n",
      "optimizer = keras.optimizers.Adam(lr=0.01)\n",
      "loss_fn = keras.losses.binary_crossentropy\n",
      "\n",
      "# Compile model\n",
      "model.compile(loss=loss_fn, optimizer=optimizer)\n",
      "\n",
      "# Preprocess purchase history data\n",
      "# TODO: define preprocessing function\n",
      "\n",
      "# Train model on preprocessed data\n",
      "X_train, y_train = train_test_split(X, y, test_size=0.2, random_state=42)\n",
      "X_train = X_train / ... # TODO: ...\n",
      "for epoch in range(num_epochs):\n",
      "  for i, x in enumerate(X_train):\n",
      "    y_pred = model.predict(x)[0]\n",
      "    nb_classes = model.predict_classes(x)\n",
      "    y_pred_idx = np.array(nb_classes)[i]\n",
      "    y_train[i] = to_categorical(y_pred_idx)\n",
      "\n",
      "  model.fit(X_train, y_train,\n",
      "    batch_size=batch_size,\n",
      "    epochs=epochs,\n",
      "\n",
      "-----------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"\n",
    "You are genius Data Scientist and Python Developer.\n",
    "Write a code that predicts the user's future 10 purchases (mcc-codes) based on his purchase history consisting of mcc codes in chronological order.\n",
    "\"\"\"\n",
    "\n",
    "answer = generate_answer(text, max_new_tokens=1000)\n",
    "\n",
    "print('-----------------------------------------------------------------------')\n",
    "print('ANSWER:', answer)\n",
    "print('-----------------------------------------------------------------------')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
